{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# PREDICT THE BURNED AREA OF FOREST FIRES WITH NEURAL NETWORKS"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "!pip install keras\n!pip install tensorflow",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Requirement already satisfied: keras in c:\\users\\91966\\anaconda3\\lib\\site-packages (2.4.3)\nRequirement already satisfied: scipy>=0.14 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from keras) (1.5.2)\nRequirement already satisfied: pyyaml in c:\\users\\91966\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\nRequirement already satisfied: numpy>=1.9.1 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from keras) (1.19.2)\nRequirement already satisfied: h5py in c:\\users\\91966\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\nRequirement already satisfied: six in c:\\users\\91966\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\nCollecting tensorflow\n  Downloading tensorflow-2.4.1-cp38-cp38-win_amd64.whl (370.7 MB)\nRequirement already satisfied: h5py~=2.10.0 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\nCollecting opt-einsum~=3.3.0\n  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\nCollecting termcolor~=1.1.0\n  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\nRequirement already satisfied: typing-extensions~=3.7.4 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from tensorflow) (3.7.4.3)\nCollecting wrapt~=1.12.1\n  Downloading wrapt-1.12.1.tar.gz (27 kB)\nCollecting google-pasta~=0.2\n  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\nCollecting gast==0.3.3\n  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\nRequirement already satisfied: numpy~=1.19.2 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from tensorflow) (1.19.2)\nRequirement already satisfied: wheel~=0.35 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from tensorflow) (0.35.1)\nCollecting astunparse~=1.6.3\n  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\nCollecting keras-preprocessing~=1.1.2\n  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\nCollecting tensorflow-estimator<2.5.0,>=2.4.0\n  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\nRequirement already satisfied: six~=1.15.0 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\nCollecting grpcio~=1.32.0\n  Downloading grpcio-1.32.0-cp38-cp38-win_amd64.whl (2.6 MB)\nCollecting protobuf>=3.9.2\n  Downloading protobuf-3.15.8-py2.py3-none-any.whl (173 kB)\nCollecting absl-py~=0.10\n  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\nCollecting flatbuffers~=1.12.0\n  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\nCollecting tensorboard~=2.4\n  Downloading tensorboard-2.5.0-py3-none-any.whl (6.0 MB)\nCollecting markdown>=2.6.8\n  Downloading Markdown-3.3.4-py3-none-any.whl (97 kB)\nCollecting tensorboard-data-server<0.7.0,>=0.6.0\n  Downloading tensorboard_data_server-0.6.0-py3-none-any.whl (2.3 kB)\nCollecting google-auth<2,>=1.6.3\n  Downloading google_auth-1.29.0-py2.py3-none-any.whl (142 kB)\nCollecting tensorboard-plugin-wit>=1.6.0\n  Downloading tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781 kB)\nRequirement already satisfied: requests<3,>=2.21.0 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (2.24.0)\nRequirement already satisfied: setuptools>=41.0.0 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (50.3.1.post20201107)\nRequirement already satisfied: werkzeug>=0.11.15 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from tensorboard~=2.4->tensorflow) (1.0.1)\nCollecting google-auth-oauthlib<0.5,>=0.4.1\n  Downloading google_auth_oauthlib-0.4.4-py2.py3-none-any.whl (18 kB)\nCollecting pyasn1-modules>=0.2.1\n  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\nCollecting rsa<5,>=3.1.4; python_version >= \"3.6\"\n  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\nCollecting cachetools<5.0,>=2.0.0\n  Downloading cachetools-4.2.1-py3-none-any.whl (12 kB)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.25.11)\nRequirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\nRequirement already satisfied: idna<3,>=2.5 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in c:\\users\\91966\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2020.6.20)\nCollecting requests-oauthlib>=0.7.0\n  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\nCollecting pyasn1<0.5.0,>=0.4.6\n  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\nCollecting oauthlib>=3.0.0\n  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\nBuilding wheels for collected packages: termcolor, wrapt\n  Building wheel for termcolor (setup.py): started\n  Building wheel for termcolor (setup.py): finished with status 'done'\n  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4835 sha256=eefdd2d83bca87f0777991b11511a3d1d87bebff4979ec81ca2d988d1677bf31\n  Stored in directory: c:\\users\\91966\\appdata\\local\\pip\\cache\\wheels\\a0\\16\\9c\\5473df82468f958445479c59e784896fa24f4a5fc024b0f501\n  Building wheel for wrapt (setup.py): started\n  Building wheel for wrapt (setup.py): finished with status 'done'\n  Created wheel for wrapt: filename=wrapt-1.12.1-py3-none-any.whl size=19558 sha256=8e30b276a52b5104530d366e4693bb08f4aac073213a885344c76fe1829374ee\n  Stored in directory: c:\\users\\91966\\appdata\\local\\pip\\cache\\wheels\\5f\\fd\\9e\\b6cf5890494cb8ef0b5eaff72e5d55a70fb56316007d6dfe73\nSuccessfully built termcolor wrapt\nInstalling collected packages: opt-einsum, termcolor, wrapt, google-pasta, gast, astunparse, keras-preprocessing, tensorflow-estimator, grpcio, protobuf, absl-py, flatbuffers, markdown, tensorboard-data-server, pyasn1, pyasn1-modules, rsa, cachetools, google-auth, tensorboard-plugin-wit, oauthlib, requests-oauthlib, google-auth-oauthlib, tensorboard, tensorflow\n  Attempting uninstall: wrapt\n    Found existing installation: wrapt 1.11.2\n    Uninstalling wrapt-1.11.2:\n      Successfully uninstalled wrapt-1.11.2\nSuccessfully installed absl-py-0.12.0 astunparse-1.6.3 cachetools-4.2.1 flatbuffers-1.12 gast-0.3.3 google-auth-1.29.0 google-auth-oauthlib-0.4.4 google-pasta-0.2.0 grpcio-1.32.0 keras-preprocessing-1.1.2 markdown-3.3.4 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.15.8 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.7.2 tensorboard-2.5.0 tensorboard-data-server-0.6.0 tensorboard-plugin-wit-1.8.0 tensorflow-2.4.1 tensorflow-estimator-2.4.0 termcolor-1.1.0 wrapt-1.12.1\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport keras\nfrom sklearn.preprocessing import StandardScaler",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df=pd.read_csv('forestfires.csv')",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "    month  day  FFMC    DMC     DC   ISI  temp  RH  wind  rain  ...  monthfeb  \\\n0     mar  fri  86.2   26.2   94.3   5.1   8.2  51   6.7   0.0  ...         0   \n1     oct  tue  90.6   35.4  669.1   6.7  18.0  33   0.9   0.0  ...         0   \n2     oct  sat  90.6   43.7  686.9   6.7  14.6  33   1.3   0.0  ...         0   \n3     mar  fri  91.7   33.3   77.5   9.0   8.3  97   4.0   0.2  ...         0   \n4     mar  sun  89.3   51.3  102.2   9.6  11.4  99   1.8   0.0  ...         0   \n..    ...  ...   ...    ...    ...   ...   ...  ..   ...   ...  ...       ...   \n512   aug  sun  81.6   56.7  665.6   1.9  27.8  32   2.7   0.0  ...         0   \n513   aug  sun  81.6   56.7  665.6   1.9  21.9  71   5.8   0.0  ...         0   \n514   aug  sun  81.6   56.7  665.6   1.9  21.2  70   6.7   0.0  ...         0   \n515   aug  sat  94.4  146.0  614.7  11.3  25.6  42   4.0   0.0  ...         0   \n516   nov  tue  79.5    3.0  106.7   1.1  11.8  31   4.5   0.0  ...         0   \n\n     monthjan  monthjul  monthjun  monthmar  monthmay  monthnov  monthoct  \\\n0           0         0         0         1         0         0         0   \n1           0         0         0         0         0         0         1   \n2           0         0         0         0         0         0         1   \n3           0         0         0         1         0         0         0   \n4           0         0         0         1         0         0         0   \n..        ...       ...       ...       ...       ...       ...       ...   \n512         0         0         0         0         0         0         0   \n513         0         0         0         0         0         0         0   \n514         0         0         0         0         0         0         0   \n515         0         0         0         0         0         0         0   \n516         0         0         0         0         0         1         0   \n\n     monthsep  size_category  \n0           0          small  \n1           0          small  \n2           0          small  \n3           0          small  \n4           0          small  \n..        ...            ...  \n512         0          large  \n513         0          large  \n514         0          large  \n515         0          small  \n516         0          small  \n\n[517 rows x 31 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>month</th>\n      <th>day</th>\n      <th>FFMC</th>\n      <th>DMC</th>\n      <th>DC</th>\n      <th>ISI</th>\n      <th>temp</th>\n      <th>RH</th>\n      <th>wind</th>\n      <th>rain</th>\n      <th>...</th>\n      <th>monthfeb</th>\n      <th>monthjan</th>\n      <th>monthjul</th>\n      <th>monthjun</th>\n      <th>monthmar</th>\n      <th>monthmay</th>\n      <th>monthnov</th>\n      <th>monthoct</th>\n      <th>monthsep</th>\n      <th>size_category</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mar</td>\n      <td>fri</td>\n      <td>86.2</td>\n      <td>26.2</td>\n      <td>94.3</td>\n      <td>5.1</td>\n      <td>8.2</td>\n      <td>51</td>\n      <td>6.7</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>small</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>oct</td>\n      <td>tue</td>\n      <td>90.6</td>\n      <td>35.4</td>\n      <td>669.1</td>\n      <td>6.7</td>\n      <td>18.0</td>\n      <td>33</td>\n      <td>0.9</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>small</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>oct</td>\n      <td>sat</td>\n      <td>90.6</td>\n      <td>43.7</td>\n      <td>686.9</td>\n      <td>6.7</td>\n      <td>14.6</td>\n      <td>33</td>\n      <td>1.3</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>small</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mar</td>\n      <td>fri</td>\n      <td>91.7</td>\n      <td>33.3</td>\n      <td>77.5</td>\n      <td>9.0</td>\n      <td>8.3</td>\n      <td>97</td>\n      <td>4.0</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>small</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mar</td>\n      <td>sun</td>\n      <td>89.3</td>\n      <td>51.3</td>\n      <td>102.2</td>\n      <td>9.6</td>\n      <td>11.4</td>\n      <td>99</td>\n      <td>1.8</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>small</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>512</th>\n      <td>aug</td>\n      <td>sun</td>\n      <td>81.6</td>\n      <td>56.7</td>\n      <td>665.6</td>\n      <td>1.9</td>\n      <td>27.8</td>\n      <td>32</td>\n      <td>2.7</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>large</td>\n    </tr>\n    <tr>\n      <th>513</th>\n      <td>aug</td>\n      <td>sun</td>\n      <td>81.6</td>\n      <td>56.7</td>\n      <td>665.6</td>\n      <td>1.9</td>\n      <td>21.9</td>\n      <td>71</td>\n      <td>5.8</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>large</td>\n    </tr>\n    <tr>\n      <th>514</th>\n      <td>aug</td>\n      <td>sun</td>\n      <td>81.6</td>\n      <td>56.7</td>\n      <td>665.6</td>\n      <td>1.9</td>\n      <td>21.2</td>\n      <td>70</td>\n      <td>6.7</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>large</td>\n    </tr>\n    <tr>\n      <th>515</th>\n      <td>aug</td>\n      <td>sat</td>\n      <td>94.4</td>\n      <td>146.0</td>\n      <td>614.7</td>\n      <td>11.3</td>\n      <td>25.6</td>\n      <td>42</td>\n      <td>4.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>small</td>\n    </tr>\n    <tr>\n      <th>516</th>\n      <td>nov</td>\n      <td>tue</td>\n      <td>79.5</td>\n      <td>3.0</td>\n      <td>106.7</td>\n      <td>1.1</td>\n      <td>11.8</td>\n      <td>31</td>\n      <td>4.5</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>small</td>\n    </tr>\n  </tbody>\n</table>\n<p>517 rows × 31 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1=df.drop(['month','day'],axis=1)",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import LabelEncoder",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lb=LabelEncoder()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df1['size_category']=lb.fit_transform(df1['size_category'])",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2=df1.values\ndf2.shape",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "(517, 29)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x=df2[:,0:28]\ny=df2[:,-1]\nx.shape",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "(517, 28)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV, KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(Dense(12, input_dim=28, activation='relu'))\nmodel.add(Dense(28, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.fit(x, y, validation_split=0.33,epochs=100, batch_size=5)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 1/100\n70/70 [==============================] - 3s 36ms/step - loss: 8.3060 - accuracy: 0.7304 - val_loss: 0.9324 - val_accuracy: 0.6316\nEpoch 2/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.8017 - accuracy: 0.7104 - val_loss: 0.7408 - val_accuracy: 0.7485\nEpoch 3/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.6512 - accuracy: 0.7408 - val_loss: 0.6563 - val_accuracy: 0.7368\nEpoch 4/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.7592 - accuracy: 0.6991 - val_loss: 0.9453 - val_accuracy: 0.7076\nEpoch 5/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.5993 - accuracy: 0.7585 - val_loss: 0.6353 - val_accuracy: 0.6550\nEpoch 6/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.5255 - accuracy: 0.8133 - val_loss: 0.5769 - val_accuracy: 0.7602\nEpoch 7/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.4688 - accuracy: 0.8128 - val_loss: 0.5626 - val_accuracy: 0.7661\nEpoch 8/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.4116 - accuracy: 0.8241 - val_loss: 0.4713 - val_accuracy: 0.7953\nEpoch 9/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.8291 - val_loss: 0.6705 - val_accuracy: 0.7661\nEpoch 10/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.5457 - accuracy: 0.7223 - val_loss: 0.6179 - val_accuracy: 0.7719\nEpoch 11/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8460 - val_loss: 0.4634 - val_accuracy: 0.7544\nEpoch 12/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8717 - val_loss: 0.4368 - val_accuracy: 0.8129\nEpoch 13/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.2906 - accuracy: 0.8779 - val_loss: 0.3815 - val_accuracy: 0.8304\nEpoch 14/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.2358 - accuracy: 0.9129 - val_loss: 0.7455 - val_accuracy: 0.7953\nEpoch 15/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.3487 - accuracy: 0.8704 - val_loss: 0.3543 - val_accuracy: 0.8304\nEpoch 16/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.2285 - accuracy: 0.9177 - val_loss: 1.0558 - val_accuracy: 0.7836\nEpoch 17/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.3862 - accuracy: 0.8686 - val_loss: 0.2641 - val_accuracy: 0.9123\nEpoch 18/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1643 - accuracy: 0.9525 - val_loss: 0.2699 - val_accuracy: 0.8713\nEpoch 19/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1550 - accuracy: 0.9543 - val_loss: 0.2781 - val_accuracy: 0.8655\nEpoch 20/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.9557 - val_loss: 0.2193 - val_accuracy: 0.9006\nEpoch 21/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1302 - accuracy: 0.9705 - val_loss: 0.2898 - val_accuracy: 0.8655\nEpoch 22/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.1044 - accuracy: 0.9602 - val_loss: 0.2226 - val_accuracy: 0.9298\nEpoch 23/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1210 - accuracy: 0.9668 - val_loss: 0.3675 - val_accuracy: 0.8480\nEpoch 24/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1155 - accuracy: 0.9551 - val_loss: 0.2560 - val_accuracy: 0.8772\nEpoch 25/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1661 - accuracy: 0.9415 - val_loss: 0.1711 - val_accuracy: 0.9298\nEpoch 26/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1464 - accuracy: 0.9488 - val_loss: 0.2597 - val_accuracy: 0.8713\nEpoch 27/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1108 - accuracy: 0.9680 - val_loss: 0.1626 - val_accuracy: 0.9181\nEpoch 28/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1345 - accuracy: 0.9497 - val_loss: 0.1585 - val_accuracy: 0.9298\nEpoch 29/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1511 - accuracy: 0.9586 - val_loss: 0.1627 - val_accuracy: 0.9415\nEpoch 30/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1636 - accuracy: 0.9506 - val_loss: 0.1512 - val_accuracy: 0.9415\nEpoch 31/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.5787 - accuracy: 0.8133 - val_loss: 0.1405 - val_accuracy: 0.9357\nEpoch 32/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.3181 - accuracy: 0.9116 - val_loss: 0.1510 - val_accuracy: 0.9240\nEpoch 33/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0901 - accuracy: 0.9749 - val_loss: 0.1376 - val_accuracy: 0.9591\nEpoch 34/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0890 - accuracy: 0.9540 - val_loss: 0.1406 - val_accuracy: 0.9415\nEpoch 35/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.1090 - accuracy: 0.9727 - val_loss: 0.1282 - val_accuracy: 0.9415\nEpoch 36/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0605 - accuracy: 0.9858 - val_loss: 0.2790 - val_accuracy: 0.8830\nEpoch 37/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0852 - accuracy: 0.9728 - val_loss: 0.1979 - val_accuracy: 0.8947\nEpoch 38/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0619 - accuracy: 0.9807 - val_loss: 0.3470 - val_accuracy: 0.8713\nEpoch 39/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0580 - accuracy: 0.9808 - val_loss: 0.1271 - val_accuracy: 0.9474\nEpoch 40/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0611 - accuracy: 0.9712 - val_loss: 0.2519 - val_accuracy: 0.9006\nEpoch 41/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9669 - val_loss: 0.2198 - val_accuracy: 0.8889\nEpoch 42/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0513 - accuracy: 0.9802 - val_loss: 0.2255 - val_accuracy: 0.9123\nEpoch 43/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9860 - val_loss: 0.1102 - val_accuracy: 0.9649\nEpoch 44/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0696 - accuracy: 0.9676 - val_loss: 0.1758 - val_accuracy: 0.9240\nEpoch 45/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0504 - accuracy: 0.9897 - val_loss: 0.1649 - val_accuracy: 0.9415\nEpoch 46/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0440 - accuracy: 0.9868 - val_loss: 0.1364 - val_accuracy: 0.9474\nEpoch 47/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.0827 - accuracy: 0.9705 - val_loss: 0.1865 - val_accuracy: 0.9123\nEpoch 48/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.9773 - val_loss: 0.2531 - val_accuracy: 0.8947\nEpoch 49/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0672 - accuracy: 0.9790 - val_loss: 0.5161 - val_accuracy: 0.8246\nEpoch 50/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.2617 - accuracy: 0.9518 - val_loss: 0.1836 - val_accuracy: 0.9181\nEpoch 51/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.9685 - val_loss: 0.0963 - val_accuracy: 0.9649\nEpoch 52/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0924 - accuracy: 0.9806 - val_loss: 0.1251 - val_accuracy: 0.9474\nEpoch 53/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0581 - accuracy: 0.9772 - val_loss: 0.1205 - val_accuracy: 0.9474\nEpoch 54/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0640 - accuracy: 0.9649 - val_loss: 0.4157 - val_accuracy: 0.8830\nEpoch 55/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1055 - accuracy: 0.9775 - val_loss: 0.2340 - val_accuracy: 0.9006\nEpoch 56/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1039 - accuracy: 0.9614 - val_loss: 0.6115 - val_accuracy: 0.8596\nEpoch 57/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1006 - accuracy: 0.9666 - val_loss: 0.1406 - val_accuracy: 0.9415\nEpoch 58/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0447 - accuracy: 0.9845 - val_loss: 0.3895 - val_accuracy: 0.8480\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "Epoch 59/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9477 - val_loss: 0.1643 - val_accuracy: 0.9415\nEpoch 60/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0591 - accuracy: 0.9787 - val_loss: 0.1113 - val_accuracy: 0.9474\nEpoch 61/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0270 - accuracy: 0.9882 - val_loss: 0.1027 - val_accuracy: 0.9591\nEpoch 62/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0380 - accuracy: 0.9834 - val_loss: 0.0843 - val_accuracy: 0.9649\nEpoch 63/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.9854 - val_loss: 0.1313 - val_accuracy: 0.9415\nEpoch 64/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9732 - val_loss: 0.0923 - val_accuracy: 0.9591\nEpoch 65/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0771 - accuracy: 0.9691 - val_loss: 0.0806 - val_accuracy: 0.9649\nEpoch 66/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0334 - accuracy: 0.9915 - val_loss: 0.0822 - val_accuracy: 0.9708\nEpoch 67/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.0444 - accuracy: 0.9838 - val_loss: 0.0889 - val_accuracy: 0.9474\nEpoch 68/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 0.9883 - val_loss: 0.1662 - val_accuracy: 0.9357\nEpoch 69/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1256 - accuracy: 0.9484 - val_loss: 0.2340 - val_accuracy: 0.9123\nEpoch 70/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0415 - accuracy: 0.9861 - val_loss: 0.1075 - val_accuracy: 0.9532\nEpoch 71/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0366 - accuracy: 0.9886 - val_loss: 0.1233 - val_accuracy: 0.9415\nEpoch 72/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1946 - accuracy: 0.9447 - val_loss: 0.1335 - val_accuracy: 0.9357\nEpoch 73/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0497 - accuracy: 0.9836 - val_loss: 0.1810 - val_accuracy: 0.9415\nEpoch 74/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0548 - accuracy: 0.9726 - val_loss: 0.2150 - val_accuracy: 0.9240\nEpoch 75/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0308 - accuracy: 0.9841 - val_loss: 0.1034 - val_accuracy: 0.9415\nEpoch 76/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9967 - val_loss: 0.0853 - val_accuracy: 0.9649\nEpoch 77/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 0.9862 - val_loss: 0.1303 - val_accuracy: 0.9415\nEpoch 78/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9803 - val_loss: 0.0926 - val_accuracy: 0.9649\nEpoch 79/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.0388 - accuracy: 0.9771 - val_loss: 0.0895 - val_accuracy: 0.9591\nEpoch 80/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0381 - accuracy: 0.9825 - val_loss: 0.2148 - val_accuracy: 0.9240\nEpoch 81/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0150 - accuracy: 0.9966 - val_loss: 0.0974 - val_accuracy: 0.9649\nEpoch 82/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.0453 - accuracy: 0.9905 - val_loss: 0.0998 - val_accuracy: 0.9591\nEpoch 83/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0194 - accuracy: 0.9958 - val_loss: 0.1246 - val_accuracy: 0.9357\nEpoch 84/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0404 - accuracy: 0.9810 - val_loss: 0.2412 - val_accuracy: 0.9181\nEpoch 85/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0660 - accuracy: 0.9771 - val_loss: 0.1745 - val_accuracy: 0.9357\nEpoch 86/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0335 - accuracy: 0.9903 - val_loss: 0.1323 - val_accuracy: 0.9474\nEpoch 87/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.0311 - accuracy: 0.9855 - val_loss: 0.1253 - val_accuracy: 0.9532\nEpoch 88/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1024 - accuracy: 0.9764 - val_loss: 0.1473 - val_accuracy: 0.9415\nEpoch 89/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0105 - accuracy: 0.9983 - val_loss: 0.2203 - val_accuracy: 0.9123\nEpoch 90/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1371 - accuracy: 0.9615 - val_loss: 0.2330 - val_accuracy: 0.9123\nEpoch 91/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0583 - accuracy: 0.9718 - val_loss: 0.1115 - val_accuracy: 0.9474\nEpoch 92/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0442 - accuracy: 0.9769 - val_loss: 0.0747 - val_accuracy: 0.9766\nEpoch 93/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 0.9914 - val_loss: 0.1770 - val_accuracy: 0.9415\nEpoch 94/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1250 - accuracy: 0.9456 - val_loss: 0.0736 - val_accuracy: 0.9708\nEpoch 95/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.0233 - accuracy: 0.9893 - val_loss: 0.1013 - val_accuracy: 0.9591\nEpoch 96/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.1806 - accuracy: 0.9636 - val_loss: 0.7880 - val_accuracy: 0.8713\nEpoch 97/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.2950 - accuracy: 0.9388 - val_loss: 0.1008 - val_accuracy: 0.9474\nEpoch 98/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 0.9844 - val_loss: 0.0702 - val_accuracy: 0.9708\nEpoch 99/100\n70/70 [==============================] - 0s 3ms/step - loss: 0.0370 - accuracy: 0.9880 - val_loss: 0.1978 - val_accuracy: 0.9357\nEpoch 100/100\n70/70 [==============================] - 0s 2ms/step - loss: 0.0299 - accuracy: 0.9869 - val_loss: 0.1181 - val_accuracy: 0.9532\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x2448fd7caf0>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scores=model.evaluate(x,y)\nprint(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "17/17 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9749\naccuracy: 97.49%\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# The dataset contains 36733 instances of 11 sensor measures aggregated over one hour (by means of average or sum) from a gas turbine."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "gs=pd.read_csv(\"gas_turbines.csv\")\ngs",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n\n           CO     NOX  \n0      3.1547  82.722  \n1      3.2363  82.776  \n2      3.2012  82.468  \n3      3.1923  82.670  \n4      3.2484  82.311  \n...       ...     ...  \n15034  4.5186  79.559  \n15035  4.8470  79.917  \n15036  7.9632  90.912  \n15037  6.2494  93.227  \n15038  4.9816  92.498  \n\n[15039 rows x 11 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>AT</th>\n      <th>AP</th>\n      <th>AH</th>\n      <th>AFDP</th>\n      <th>GTEP</th>\n      <th>TIT</th>\n      <th>TAT</th>\n      <th>TEY</th>\n      <th>CDP</th>\n      <th>CO</th>\n      <th>NOX</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6.8594</td>\n      <td>1007.9</td>\n      <td>96.799</td>\n      <td>3.5000</td>\n      <td>19.663</td>\n      <td>1059.2</td>\n      <td>550.00</td>\n      <td>114.70</td>\n      <td>10.605</td>\n      <td>3.1547</td>\n      <td>82.722</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>6.7850</td>\n      <td>1008.4</td>\n      <td>97.118</td>\n      <td>3.4998</td>\n      <td>19.728</td>\n      <td>1059.3</td>\n      <td>550.00</td>\n      <td>114.72</td>\n      <td>10.598</td>\n      <td>3.2363</td>\n      <td>82.776</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6.8977</td>\n      <td>1008.8</td>\n      <td>95.939</td>\n      <td>3.4824</td>\n      <td>19.779</td>\n      <td>1059.4</td>\n      <td>549.87</td>\n      <td>114.71</td>\n      <td>10.601</td>\n      <td>3.2012</td>\n      <td>82.468</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>7.0569</td>\n      <td>1009.2</td>\n      <td>95.249</td>\n      <td>3.4805</td>\n      <td>19.792</td>\n      <td>1059.6</td>\n      <td>549.99</td>\n      <td>114.72</td>\n      <td>10.606</td>\n      <td>3.1923</td>\n      <td>82.670</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.3978</td>\n      <td>1009.7</td>\n      <td>95.150</td>\n      <td>3.4976</td>\n      <td>19.765</td>\n      <td>1059.7</td>\n      <td>549.98</td>\n      <td>114.72</td>\n      <td>10.612</td>\n      <td>3.2484</td>\n      <td>82.311</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>15034</th>\n      <td>9.0301</td>\n      <td>1005.6</td>\n      <td>98.460</td>\n      <td>3.5421</td>\n      <td>19.164</td>\n      <td>1049.7</td>\n      <td>546.21</td>\n      <td>111.61</td>\n      <td>10.400</td>\n      <td>4.5186</td>\n      <td>79.559</td>\n    </tr>\n    <tr>\n      <th>15035</th>\n      <td>7.8879</td>\n      <td>1005.9</td>\n      <td>99.093</td>\n      <td>3.5059</td>\n      <td>19.414</td>\n      <td>1046.3</td>\n      <td>543.22</td>\n      <td>111.78</td>\n      <td>10.433</td>\n      <td>4.8470</td>\n      <td>79.917</td>\n    </tr>\n    <tr>\n      <th>15036</th>\n      <td>7.2647</td>\n      <td>1006.3</td>\n      <td>99.496</td>\n      <td>3.4770</td>\n      <td>19.530</td>\n      <td>1037.7</td>\n      <td>537.32</td>\n      <td>110.19</td>\n      <td>10.483</td>\n      <td>7.9632</td>\n      <td>90.912</td>\n    </tr>\n    <tr>\n      <th>15037</th>\n      <td>7.0060</td>\n      <td>1006.8</td>\n      <td>99.008</td>\n      <td>3.4486</td>\n      <td>19.377</td>\n      <td>1043.2</td>\n      <td>541.24</td>\n      <td>110.74</td>\n      <td>10.533</td>\n      <td>6.2494</td>\n      <td>93.227</td>\n    </tr>\n    <tr>\n      <th>15038</th>\n      <td>6.9279</td>\n      <td>1007.2</td>\n      <td>97.533</td>\n      <td>3.4275</td>\n      <td>19.306</td>\n      <td>1049.9</td>\n      <td>545.85</td>\n      <td>111.58</td>\n      <td>10.583</td>\n      <td>4.9816</td>\n      <td>92.498</td>\n    </tr>\n  </tbody>\n</table>\n<p>15039 rows × 11 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "gs1=gs.values\ngs1",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n          82.722 ],\n       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n          82.776 ],\n       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n          82.468 ],\n       ...,\n       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n          90.912 ],\n       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n          93.227 ],\n       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n          92.498 ]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X=gs1[:,[0,1,2,3,4,5,6,8,9,10]]\nY=gs1[:,-4]\nX",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "array([[   6.8594, 1007.9   ,   96.799 , ...,   10.605 ,    3.1547,\n          82.722 ],\n       [   6.785 , 1008.4   ,   97.118 , ...,   10.598 ,    3.2363,\n          82.776 ],\n       [   6.8977, 1008.8   ,   95.939 , ...,   10.601 ,    3.2012,\n          82.468 ],\n       ...,\n       [   7.2647, 1006.3   ,   99.496 , ...,   10.483 ,    7.9632,\n          90.912 ],\n       [   7.006 , 1006.8   ,   99.008 , ...,   10.533 ,    6.2494,\n          93.227 ],\n       [   6.9279, 1007.2   ,   97.533 , ...,   10.583 ,    4.9816,\n          92.498 ]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.25,random_state=101)",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import MinMaxScaler",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scaler=MinMaxScaler()\nscaler.fit(x_train)\n",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 22,
          "data": {
            "text/plain": "MinMaxScaler()"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "x_train=scaler.transform(x_train)\nx_test=scaler.transform(x_test)\nx_test",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 23,
          "data": {
            "text/plain": "array([[0.35890393, 0.40602285, 0.91801706, ..., 0.34107329, 0.03084967,\n        0.48475958],\n       [0.55162803, 0.59086189, 0.72785444, ..., 0.42819611, 0.02833486,\n        0.43366477],\n       [0.69430373, 0.53478712, 0.55215014, ..., 0.14847583, 0.15186537,\n        0.33822331],\n       ...,\n       [0.29923532, 0.48494289, 0.94876603, ..., 0.77514199, 0.00101504,\n        0.41400706],\n       [0.64399376, 0.35825545, 0.50904718, ..., 0.04705791, 0.10100297,\n        0.36756316],\n       [0.3486443 , 0.24340602, 0.81637941, ..., 0.34416412, 0.00787964,\n        0.54170062]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = Sequential()\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\nmodel.add(Dense(10, activation='relu'))\n# add nodes for prediction\nmodel.add(Dense(1))",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model.compile(optimizer='rmsprop',loss='mse')",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Fit the model\nmodel.fit(x_train, y_train, epochs=250)",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 1/250\n353/353 [==============================] - 1s 893us/step - loss: 16706.4358\nEpoch 2/250\n353/353 [==============================] - 0s 843us/step - loss: 1280.1191\nEpoch 3/250\n353/353 [==============================] - 0s 840us/step - loss: 34.5055\nEpoch 4/250\n353/353 [==============================] - 0s 852us/step - loss: 24.4014\nEpoch 5/250\n353/353 [==============================] - 0s 853us/step - loss: 17.3246\nEpoch 6/250\n353/353 [==============================] - 0s 834us/step - loss: 12.6862\nEpoch 7/250\n353/353 [==============================] - 0s 825us/step - loss: 8.9782\nEpoch 8/250\n353/353 [==============================] - 0s 962us/step - loss: 6.9313\nEpoch 9/250\n353/353 [==============================] - 0s 870us/step - loss: 5.1744\nEpoch 10/250\n353/353 [==============================] - 0s 832us/step - loss: 4.5002\nEpoch 11/250\n353/353 [==============================] - 0s 841us/step - loss: 4.0524\nEpoch 12/250\n353/353 [==============================] - 0s 832us/step - loss: 3.6507\nEpoch 13/250\n353/353 [==============================] - 0s 843us/step - loss: 3.4939\nEpoch 14/250\n353/353 [==============================] - 0s 870us/step - loss: 3.1293\nEpoch 15/250\n353/353 [==============================] - 0s 885us/step - loss: 3.0100\nEpoch 16/250\n353/353 [==============================] - 0s 850us/step - loss: 2.8349\nEpoch 17/250\n353/353 [==============================] - 0s 900us/step - loss: 2.6941\nEpoch 18/250\n353/353 [==============================] - 0s 831us/step - loss: 2.6030\nEpoch 19/250\n353/353 [==============================] - 0s 844us/step - loss: 2.4846\nEpoch 20/250\n353/353 [==============================] - 0s 874us/step - loss: 2.3664\nEpoch 21/250\n353/353 [==============================] - 0s 850us/step - loss: 2.2680\nEpoch 22/250\n353/353 [==============================] - 0s 832us/step - loss: 2.2288\nEpoch 23/250\n353/353 [==============================] - 0s 832us/step - loss: 2.1111\nEpoch 24/250\n353/353 [==============================] - 0s 851us/step - loss: 2.0341\nEpoch 25/250\n353/353 [==============================] - 0s 847us/step - loss: 1.9053\nEpoch 26/250\n353/353 [==============================] - 0s 907us/step - loss: 1.8013\nEpoch 27/250\n353/353 [==============================] - 0s 826us/step - loss: 1.7636\nEpoch 28/250\n353/353 [==============================] - 0s 841us/step - loss: 1.7732\nEpoch 29/250\n353/353 [==============================] - 0s 858us/step - loss: 1.7523\nEpoch 30/250\n353/353 [==============================] - 0s 845us/step - loss: 1.6564\nEpoch 31/250\n353/353 [==============================] - 0s 833us/step - loss: 1.5810\nEpoch 32/250\n353/353 [==============================] - 0s 907us/step - loss: 1.5874\nEpoch 33/250\n353/353 [==============================] - 0s 878us/step - loss: 1.4952\nEpoch 34/250\n353/353 [==============================] - 0s 857us/step - loss: 1.4707\nEpoch 35/250\n353/353 [==============================] - 0s 1ms/step - loss: 1.3887\nEpoch 36/250\n353/353 [==============================] - 0s 951us/step - loss: 1.3629\nEpoch 37/250\n353/353 [==============================] - 0s 1ms/step - loss: 1.3574\nEpoch 38/250\n353/353 [==============================] - 0s 979us/step - loss: 1.3650\nEpoch 39/250\n353/353 [==============================] - 0s 995us/step - loss: 1.2807\nEpoch 40/250\n353/353 [==============================] - 0s 841us/step - loss: 1.3020\nEpoch 41/250\n353/353 [==============================] - 0s 837us/step - loss: 1.1754\nEpoch 42/250\n353/353 [==============================] - 0s 884us/step - loss: 1.1707\nEpoch 43/250\n353/353 [==============================] - 0s 994us/step - loss: 1.1628\nEpoch 44/250\n353/353 [==============================] - 0s 841us/step - loss: 1.1315\nEpoch 45/250\n353/353 [==============================] - 0s 921us/step - loss: 1.0997\nEpoch 46/250\n353/353 [==============================] - 0s 960us/step - loss: 1.1120 0s - loss: \nEpoch 47/250\n353/353 [==============================] - 0s 823us/step - loss: 1.1472\nEpoch 48/250\n353/353 [==============================] - 0s 833us/step - loss: 1.1308\nEpoch 49/250\n353/353 [==============================] - 0s 916us/step - loss: 1.1021\nEpoch 50/250\n353/353 [==============================] - 0s 918us/step - loss: 1.0555\nEpoch 51/250\n353/353 [==============================] - 0s 997us/step - loss: 1.0896\nEpoch 52/250\n353/353 [==============================] - 0s 920us/step - loss: 1.0231\nEpoch 53/250\n353/353 [==============================] - 0s 844us/step - loss: 1.0371\nEpoch 54/250\n353/353 [==============================] - 0s 824us/step - loss: 1.0551\nEpoch 55/250\n353/353 [==============================] - 0s 819us/step - loss: 1.0351\nEpoch 56/250\n353/353 [==============================] - 0s 820us/step - loss: 0.9855\nEpoch 57/250\n353/353 [==============================] - 0s 835us/step - loss: 1.0532\nEpoch 58/250\n353/353 [==============================] - 0s 1ms/step - loss: 1.0149\nEpoch 59/250\n353/353 [==============================] - 0s 803us/step - loss: 1.0190\nEpoch 60/250\n353/353 [==============================] - 0s 899us/step - loss: 1.0129\nEpoch 61/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.9559\nEpoch 62/250\n353/353 [==============================] - 0s 846us/step - loss: 1.0196\nEpoch 63/250\n353/353 [==============================] - 0s 854us/step - loss: 0.9814\nEpoch 64/250\n353/353 [==============================] - 0s 828us/step - loss: 0.9728\nEpoch 65/250\n353/353 [==============================] - 0s 854us/step - loss: 0.9702\nEpoch 66/250\n353/353 [==============================] - 0s 846us/step - loss: 0.9556\nEpoch 67/250\n353/353 [==============================] - 0s 872us/step - loss: 1.0009\nEpoch 68/250\n353/353 [==============================] - 0s 837us/step - loss: 0.9528\nEpoch 69/250\n353/353 [==============================] - 0s 859us/step - loss: 0.9465\nEpoch 70/250\n353/353 [==============================] - 0s 847us/step - loss: 0.9363\nEpoch 71/250\n353/353 [==============================] - 0s 979us/step - loss: 0.9277\nEpoch 72/250\n353/353 [==============================] - 0s 882us/step - loss: 0.9376\nEpoch 73/250\n353/353 [==============================] - 0s 836us/step - loss: 0.9332\nEpoch 74/250\n353/353 [==============================] - 0s 953us/step - loss: 0.9683\nEpoch 75/250\n353/353 [==============================] - 0s 874us/step - loss: 0.9249\nEpoch 76/250\n353/353 [==============================] - 0s 861us/step - loss: 0.9283\nEpoch 77/250\n353/353 [==============================] - 0s 833us/step - loss: 0.9725\nEpoch 78/250\n353/353 [==============================] - 0s 840us/step - loss: 0.9351\nEpoch 79/250\n353/353 [==============================] - 0s 896us/step - loss: 0.9356\nEpoch 80/250\n353/353 [==============================] - 0s 857us/step - loss: 0.9041\nEpoch 81/250\n353/353 [==============================] - 0s 799us/step - loss: 0.9249\nEpoch 82/250\n353/353 [==============================] - 0s 829us/step - loss: 0.9155\nEpoch 83/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8727\nEpoch 84/250\n353/353 [==============================] - 0s 860us/step - loss: 0.9766\nEpoch 85/250\n353/353 [==============================] - 0s 838us/step - loss: 0.9044\nEpoch 86/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8995\nEpoch 87/250\n353/353 [==============================] - 0s 961us/step - loss: 0.9067\nEpoch 88/250\n353/353 [==============================] - 0s 849us/step - loss: 0.9010\nEpoch 89/250\n353/353 [==============================] - 0s 896us/step - loss: 0.8929\nEpoch 90/250\n353/353 [==============================] - 0s 848us/step - loss: 0.9047\nEpoch 91/250\n353/353 [==============================] - 0s 844us/step - loss: 0.9437\nEpoch 92/250\n353/353 [==============================] - 0s 833us/step - loss: 0.8850\nEpoch 93/250\n353/353 [==============================] - 0s 849us/step - loss: 0.8799\nEpoch 94/250\n353/353 [==============================] - 0s 874us/step - loss: 0.9042\nEpoch 95/250\n353/353 [==============================] - 0s 904us/step - loss: 0.8859\nEpoch 96/250\n353/353 [==============================] - 0s 840us/step - loss: 0.9054\nEpoch 97/250\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "353/353 [==============================] - 0s 825us/step - loss: 0.9108\nEpoch 98/250\n353/353 [==============================] - 0s 816us/step - loss: 0.8730\nEpoch 99/250\n353/353 [==============================] - 0s 817us/step - loss: 0.8409\nEpoch 100/250\n353/353 [==============================] - 0s 819us/step - loss: 0.8840\nEpoch 101/250\n353/353 [==============================] - 0s 819us/step - loss: 0.8409\nEpoch 102/250\n353/353 [==============================] - 0s 856us/step - loss: 0.8523\nEpoch 103/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.9104\nEpoch 104/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8995\nEpoch 105/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8817\nEpoch 106/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8561\nEpoch 107/250\n353/353 [==============================] - 0s 815us/step - loss: 0.8509\nEpoch 108/250\n353/353 [==============================] - 0s 893us/step - loss: 0.8602\nEpoch 109/250\n353/353 [==============================] - 0s 840us/step - loss: 0.8482\nEpoch 110/250\n353/353 [==============================] - 0s 868us/step - loss: 0.8710\nEpoch 111/250\n353/353 [==============================] - 0s 811us/step - loss: 0.8475\nEpoch 112/250\n353/353 [==============================] - 0s 935us/step - loss: 0.8491\nEpoch 113/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8647\nEpoch 114/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8237\nEpoch 115/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8609\nEpoch 116/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8630\nEpoch 117/250\n353/353 [==============================] - 0s 995us/step - loss: 0.8565\nEpoch 118/250\n353/353 [==============================] - 0s 975us/step - loss: 0.8440\nEpoch 119/250\n353/353 [==============================] - 0s 852us/step - loss: 0.8707\nEpoch 120/250\n353/353 [==============================] - 0s 833us/step - loss: 0.8430\nEpoch 121/250\n353/353 [==============================] - 0s 986us/step - loss: 0.8474\nEpoch 122/250\n353/353 [==============================] - 0s 853us/step - loss: 0.8307\nEpoch 123/250\n353/353 [==============================] - 0s 867us/step - loss: 0.8471\nEpoch 124/250\n353/353 [==============================] - 0s 969us/step - loss: 0.8470\nEpoch 125/250\n353/353 [==============================] - 0s 857us/step - loss: 0.8242\nEpoch 126/250\n353/353 [==============================] - 0s 841us/step - loss: 0.8930\nEpoch 127/250\n353/353 [==============================] - 0s 921us/step - loss: 0.8154\nEpoch 128/250\n353/353 [==============================] - 0s 908us/step - loss: 0.8247 0s - loss: \nEpoch 129/250\n353/353 [==============================] - 0s 918us/step - loss: 0.8503\nEpoch 130/250\n353/353 [==============================] - 0s 885us/step - loss: 0.8396\nEpoch 131/250\n353/353 [==============================] - 0s 838us/step - loss: 0.8462\nEpoch 132/250\n353/353 [==============================] - 0s 870us/step - loss: 0.7911\nEpoch 133/250\n353/353 [==============================] - 0s 876us/step - loss: 0.8210\nEpoch 134/250\n353/353 [==============================] - 0s 833us/step - loss: 0.8365\nEpoch 135/250\n353/353 [==============================] - 0s 822us/step - loss: 0.8607\nEpoch 136/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.7962\nEpoch 137/250\n353/353 [==============================] - 0s 917us/step - loss: 0.8407\nEpoch 138/250\n353/353 [==============================] - 0s 906us/step - loss: 0.8111\nEpoch 139/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8526\nEpoch 140/250\n353/353 [==============================] - 0s 943us/step - loss: 0.8178\nEpoch 141/250\n353/353 [==============================] - 0s 847us/step - loss: 0.8623\nEpoch 142/250\n353/353 [==============================] - 0s 817us/step - loss: 0.8006\nEpoch 143/250\n353/353 [==============================] - 0s 858us/step - loss: 0.8193\nEpoch 144/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8201\nEpoch 145/250\n353/353 [==============================] - 0s 869us/step - loss: 0.8142\nEpoch 146/250\n353/353 [==============================] - 0s 836us/step - loss: 0.8138\nEpoch 147/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8098\nEpoch 148/250\n353/353 [==============================] - 0s 805us/step - loss: 0.8389\nEpoch 149/250\n353/353 [==============================] - 0s 855us/step - loss: 0.8206\nEpoch 150/250\n353/353 [==============================] - 0s 851us/step - loss: 0.8409\nEpoch 151/250\n353/353 [==============================] - 0s 812us/step - loss: 0.8360\nEpoch 152/250\n353/353 [==============================] - 0s 853us/step - loss: 0.8138\nEpoch 153/250\n353/353 [==============================] - 0s 871us/step - loss: 0.8478\nEpoch 154/250\n353/353 [==============================] - 0s 955us/step - loss: 0.8128\nEpoch 155/250\n353/353 [==============================] - 0s 867us/step - loss: 0.8295\nEpoch 156/250\n353/353 [==============================] - 0s 819us/step - loss: 0.7986\nEpoch 157/250\n353/353 [==============================] - 0s 810us/step - loss: 0.8299\nEpoch 158/250\n353/353 [==============================] - 0s 816us/step - loss: 0.8308\nEpoch 159/250\n353/353 [==============================] - 0s 938us/step - loss: 0.7951\nEpoch 160/250\n353/353 [==============================] - 0s 825us/step - loss: 0.7998\nEpoch 161/250\n353/353 [==============================] - 0s 884us/step - loss: 0.7966\nEpoch 162/250\n353/353 [==============================] - 0s 949us/step - loss: 0.8105\nEpoch 163/250\n353/353 [==============================] - 0s 856us/step - loss: 0.8613\nEpoch 164/250\n353/353 [==============================] - 0s 847us/step - loss: 0.7999\nEpoch 165/250\n353/353 [==============================] - 0s 867us/step - loss: 0.8178\nEpoch 166/250\n353/353 [==============================] - 0s 918us/step - loss: 0.8414\nEpoch 167/250\n353/353 [==============================] - 0s 938us/step - loss: 0.7792\nEpoch 168/250\n353/353 [==============================] - 0s 890us/step - loss: 0.7885\nEpoch 169/250\n353/353 [==============================] - 0s 882us/step - loss: 0.8192\nEpoch 170/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8290\nEpoch 171/250\n353/353 [==============================] - 0s 980us/step - loss: 0.8018\nEpoch 172/250\n353/353 [==============================] - 0s 887us/step - loss: 0.8445\nEpoch 173/250\n353/353 [==============================] - 0s 984us/step - loss: 0.8203\nEpoch 174/250\n353/353 [==============================] - 0s 877us/step - loss: 0.7936\nEpoch 175/250\n353/353 [==============================] - 0s 890us/step - loss: 0.7929\nEpoch 176/250\n353/353 [==============================] - 0s 993us/step - loss: 0.9059\nEpoch 177/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8055\nEpoch 178/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.7899\nEpoch 179/250\n353/353 [==============================] - 0s 812us/step - loss: 0.7555\nEpoch 180/250\n353/353 [==============================] - 0s 888us/step - loss: 0.8037\nEpoch 181/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.8042\nEpoch 182/250\n353/353 [==============================] - 0s 834us/step - loss: 0.8030\nEpoch 183/250\n353/353 [==============================] - 0s 820us/step - loss: 0.8110\nEpoch 184/250\n353/353 [==============================] - 0s 926us/step - loss: 0.8036\nEpoch 185/250\n353/353 [==============================] - 0s 829us/step - loss: 0.7991\nEpoch 186/250\n353/353 [==============================] - 0s 879us/step - loss: 0.8256\nEpoch 187/250\n353/353 [==============================] - 0s 932us/step - loss: 0.8128\nEpoch 188/250\n353/353 [==============================] - 0s 873us/step - loss: 0.7787\nEpoch 189/250\n353/353 [==============================] - 0s 902us/step - loss: 0.7769\nEpoch 190/250\n353/353 [==============================] - 0s 1ms/step - loss: 0.7650\nEpoch 191/250\n353/353 [==============================] - 0s 985us/step - loss: 0.7677\nEpoch 192/250\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "353/353 [==============================] - 0s 889us/step - loss: 0.8151\nEpoch 193/250\n353/353 [==============================] - 0s 874us/step - loss: 0.7705\nEpoch 194/250\n353/353 [==============================] - 0s 917us/step - loss: 0.7844\nEpoch 195/250\n353/353 [==============================] - 0s 896us/step - loss: 0.7894\nEpoch 196/250\n353/353 [==============================] - 0s 808us/step - loss: 0.8112\nEpoch 197/250\n353/353 [==============================] - 0s 825us/step - loss: 0.8000\nEpoch 198/250\n353/353 [==============================] - 0s 824us/step - loss: 0.8093\nEpoch 199/250\n353/353 [==============================] - 0s 817us/step - loss: 0.7885\nEpoch 200/250\n353/353 [==============================] - 0s 830us/step - loss: 0.8070\nEpoch 201/250\n353/353 [==============================] - 0s 840us/step - loss: 0.7753\nEpoch 202/250\n353/353 [==============================] - 0s 832us/step - loss: 0.8238\nEpoch 203/250\n353/353 [==============================] - 0s 948us/step - loss: 0.7863\nEpoch 204/250\n353/353 [==============================] - 0s 827us/step - loss: 0.8091\nEpoch 205/250\n353/353 [==============================] - 0s 816us/step - loss: 0.8166\nEpoch 206/250\n353/353 [==============================] - 0s 925us/step - loss: 0.8259\nEpoch 207/250\n353/353 [==============================] - 0s 815us/step - loss: 0.7738\nEpoch 208/250\n353/353 [==============================] - 0s 811us/step - loss: 0.7865\nEpoch 209/250\n353/353 [==============================] - 0s 829us/step - loss: 0.8343\nEpoch 210/250\n353/353 [==============================] - 0s 829us/step - loss: 0.8256\nEpoch 211/250\n353/353 [==============================] - 0s 928us/step - loss: 0.7917\nEpoch 212/250\n353/353 [==============================] - 0s 897us/step - loss: 0.7827\nEpoch 213/250\n353/353 [==============================] - 0s 830us/step - loss: 0.7862\nEpoch 214/250\n353/353 [==============================] - 0s 841us/step - loss: 0.8096\nEpoch 215/250\n353/353 [==============================] - 0s 870us/step - loss: 0.7795\nEpoch 216/250\n353/353 [==============================] - 0s 820us/step - loss: 0.7877\nEpoch 217/250\n353/353 [==============================] - 0s 844us/step - loss: 0.7659\nEpoch 218/250\n353/353 [==============================] - 0s 834us/step - loss: 0.7623\nEpoch 219/250\n353/353 [==============================] - 0s 813us/step - loss: 0.7996\nEpoch 220/250\n353/353 [==============================] - 0s 956us/step - loss: 0.7927\nEpoch 221/250\n353/353 [==============================] - 0s 856us/step - loss: 0.7917\nEpoch 222/250\n353/353 [==============================] - 0s 969us/step - loss: 0.7855\nEpoch 223/250\n353/353 [==============================] - 0s 960us/step - loss: 0.7970\nEpoch 224/250\n353/353 [==============================] - 0s 955us/step - loss: 0.7861\nEpoch 225/250\n353/353 [==============================] - 0s 902us/step - loss: 0.7952\nEpoch 226/250\n353/353 [==============================] - 0s 809us/step - loss: 0.8697\nEpoch 227/250\n353/353 [==============================] - 0s 828us/step - loss: 0.7918\nEpoch 228/250\n353/353 [==============================] - 0s 893us/step - loss: 0.7545\nEpoch 229/250\n353/353 [==============================] - 0s 888us/step - loss: 0.7944\nEpoch 230/250\n353/353 [==============================] - 0s 828us/step - loss: 0.7982\nEpoch 231/250\n353/353 [==============================] - 0s 885us/step - loss: 0.7527\nEpoch 232/250\n353/353 [==============================] - 0s 892us/step - loss: 0.7889\nEpoch 233/250\n353/353 [==============================] - 0s 812us/step - loss: 0.7886\nEpoch 234/250\n353/353 [==============================] - 0s 878us/step - loss: 0.7430\nEpoch 235/250\n353/353 [==============================] - 0s 858us/step - loss: 0.7819\nEpoch 236/250\n353/353 [==============================] - 0s 839us/step - loss: 0.7652\nEpoch 237/250\n353/353 [==============================] - 0s 967us/step - loss: 0.7538\nEpoch 238/250\n353/353 [==============================] - 0s 814us/step - loss: 0.7678\nEpoch 239/250\n353/353 [==============================] - 0s 816us/step - loss: 0.7608\nEpoch 240/250\n353/353 [==============================] - 0s 970us/step - loss: 0.7858\nEpoch 241/250\n353/353 [==============================] - 0s 815us/step - loss: 0.7588\nEpoch 242/250\n353/353 [==============================] - 0s 812us/step - loss: 0.7710\nEpoch 243/250\n353/353 [==============================] - 0s 872us/step - loss: 0.7939\nEpoch 244/250\n353/353 [==============================] - 0s 901us/step - loss: 0.7758\nEpoch 245/250\n353/353 [==============================] - 0s 906us/step - loss: 0.7382\nEpoch 246/250\n353/353 [==============================] - 0s 911us/step - loss: 0.7574\nEpoch 247/250\n353/353 [==============================] - 0s 816us/step - loss: 0.7632\nEpoch 248/250\n353/353 [==============================] - 0s 867us/step - loss: 0.7797\nEpoch 249/250\n353/353 [==============================] - 0s 907us/step - loss: 0.7940\nEpoch 250/250\n353/353 [==============================] - 0s 811us/step - loss: 0.8093\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 26,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x24492826c40>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "%matplotlib inline\nimport matplotlib as plot\nmodel_loss = pd.DataFrame(model.history.history)\nmodel_loss.plot()",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 27,
          "data": {
            "text/plain": "<AxesSubplot:>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred = model.predict(x_test)",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 29,
          "data": {
            "text/plain": "array([[131.20926],\n       [132.3917 ],\n       [109.84866],\n       ...,\n       [158.75648],\n       [103.47058],\n       [131.48462]], dtype=float32)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "pred = pred.ravel()",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "test_score = model.evaluate(x_test,y_test,verbose=0)\ntest_score",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "2.381887674331665"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import mean_absolute_error,mean_squared_error",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "mean_absolute_error(pred,y_test)",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": "1.4094993262595321"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "mean_squared_error(pred,y_test)",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 34,
          "data": {
            "text/plain": "2.381887263458778"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plt.scatter(y_test,pred)",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "<matplotlib.collections.PathCollection at 0x244949d7370>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 432x288 with 1 Axes>",
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "gist": {
      "id": "05607e85e8da05f57becd8ff1045eeb2",
      "data": {
        "description": " neural networking assigment.ipynb",
        "public": true
      }
    },
    "_draft": {
      "nbviewer_url": "https://gist.github.com/05607e85e8da05f57becd8ff1045eeb2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}